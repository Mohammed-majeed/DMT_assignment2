{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\n","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:03.492319Z","iopub.execute_input":"2023-05-11T14:41:03.494671Z","iopub.status.idle":"2023-05-11T14:41:03.504822Z","shell.execute_reply.started":"2023-05-11T14:41:03.494566Z","shell.execute_reply":"2023-05-11T14:41:03.502526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-11T14:41:03.517803Z","iopub.execute_input":"2023-05-11T14:41:03.518737Z","iopub.status.idle":"2023-05-11T14:41:03.530920Z","shell.execute_reply.started":"2023-05-11T14:41:03.518684Z","shell.execute_reply":"2023-05-11T14:41:03.529617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:03.538515Z","iopub.execute_input":"2023-05-11T14:41:03.538998Z","iopub.status.idle":"2023-05-11T14:41:03.545405Z","shell.execute_reply.started":"2023-05-11T14:41:03.538955Z","shell.execute_reply":"2023-05-11T14:41:03.543702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the data\ndf_original = pd.read_csv('/kaggle/input/vu-dmt-assigment-2-2023/training_set_VU_DM.csv',nrows=100000)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:03.563591Z","iopub.execute_input":"2023-05-11T14:41:03.564484Z","iopub.status.idle":"2023-05-11T14:41:04.436998Z","shell.execute_reply.started":"2023-05-11T14:41:03.564438Z","shell.execute_reply":"2023-05-11T14:41:04.435726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample 10000 instances of the data\ndf = df_original#.sample(n = 50000)\nprint(\"the shape of the data \",df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:04.439100Z","iopub.execute_input":"2023-05-11T14:41:04.439559Z","iopub.status.idle":"2023-05-11T14:41:04.500976Z","shell.execute_reply.started":"2023-05-11T14:41:04.439518Z","shell.execute_reply":"2023-05-11T14:41:04.499631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values\ndf.isnull().sum(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:04.502921Z","iopub.execute_input":"2023-05-11T14:41:04.503899Z","iopub.status.idle":"2023-05-11T14:41:04.539525Z","shell.execute_reply.started":"2023-05-11T14:41:04.503857Z","shell.execute_reply":"2023-05-11T14:41:04.538505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import missingno as msno\n# visualize the missing values\nmsno.bar(df)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:04.542892Z","iopub.execute_input":"2023-05-11T14:41:04.544205Z","iopub.status.idle":"2023-05-11T14:41:07.717200Z","shell.execute_reply.started":"2023-05-11T14:41:04.544157Z","shell.execute_reply":"2023-05-11T14:41:07.715737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:07.718968Z","iopub.execute_input":"2023-05-11T14:41:07.719434Z","iopub.status.idle":"2023-05-11T14:41:08.087299Z","shell.execute_reply.started":"2023-05-11T14:41:07.719389Z","shell.execute_reply":"2023-05-11T14:41:08.085943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the distribution of the booking_bool variable\nprint(f\"Check the distribution of the booking_bool variable \\n{df['booking_bool'].value_counts()}\\n\")\n\n# Check the distribution of the click_bool variable\nprint(f\"Check the distribution of the click_bool variable \\n{df['click_bool'].value_counts()}\\n\")\n\n# Check the distribution of the visitor_location_country_id \nprint(f\"Check the distribution of the visitor_location_country_id \\n{df['visitor_location_country_id'].value_counts().head()}\\n\")\n\n# Check the distribution of the visitor_hist_starrating \nprint(f\"Check the distribution of the visitor_hist_starrating \\n{df['visitor_hist_starrating'].value_counts().head()}\\n\")\n\n# average visitor_hist_starrating for each srch_id\nprint(f\"average visitor_hist_starrating for each srch_id \\n{df.groupby('srch_id')['visitor_hist_starrating'].mean().dropna().head()}\\n\")\n\n# average visitor_location_country_id for each srch_id\nprint(f\"average visitor_location_country_id for each srch_id \\n{df.groupby('srch_id')['visitor_location_country_id'].mean().dropna().head()}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:08.089378Z","iopub.execute_input":"2023-05-11T14:41:08.090263Z","iopub.status.idle":"2023-05-11T14:41:08.115919Z","shell.execute_reply.started":"2023-05-11T14:41:08.090205Z","shell.execute_reply":"2023-05-11T14:41:08.114515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have already loaded the data into a DataFrame called 'df'\ndf['date_time'] = pd.to_datetime(df['date_time'])  # Convert 'date_time' column to datetime format\ndf_sorted = df.sort_values('date_time')  # Sort the DataFrame based on 'date_time'\n\n# Plotting the sorted data\nplt.figure(figsize=(10,5))\nplt.plot(df_sorted['date_time'], df_sorted['price_usd'])\nplt.xlabel('Date')\nplt.ylabel('Price')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:08.117833Z","iopub.execute_input":"2023-05-11T14:41:08.118582Z","iopub.status.idle":"2023-05-11T14:41:08.488628Z","shell.execute_reply.started":"2023-05-11T14:41:08.118538Z","shell.execute_reply":"2023-05-11T14:41:08.487559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Visualize the distribution of numerical features\n# num_features = ['booking_bool','visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_starrating', \n#                 'prop_review_score', 'prop_location_score1', 'prop_location_score2', \n#                 'prop_log_historical_price', 'position', 'price_usd',\n#                 'srch_query_affinity_score', 'orig_destination_distance',\n#                 'comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', \n#                 'comp2_rate', 'comp2_inv', 'comp2_rate_percent_diff', \n#                 'comp3_rate', 'comp3_inv', 'comp3_rate_percent_diff', 'comp4_rate',\n#                 'comp4_inv', 'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv', \n#                 'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv', \n#                 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', \n#                 'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n#                 'comp8_rate_percent_diff']\n\n# fig, axes = plt.subplots(9, 4, figsize=(20, 30))\n# axes = axes.flatten()\n\n# for i, feature in enumerate(num_features):\n#     sns.histplot(df[feature], kde=False, ax=axes[i])\n#     axes[i].set_title(feature)\n\n# plt.tight_layout()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:08.490257Z","iopub.execute_input":"2023-05-11T14:41:08.490900Z","iopub.status.idle":"2023-05-11T14:41:08.496377Z","shell.execute_reply.started":"2023-05-11T14:41:08.490860Z","shell.execute_reply":"2023-05-11T14:41:08.495435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the correlation between features\n# Create a correlation matrix\ncorr = df.corr()\n# Set up the plot\nfig, ax = plt.subplots(figsize=(12, 10))\n# Create a heatmap of the correlation matrix\nsns.heatmap(corr, cmap='coolwarm', ax=ax)\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:08.497920Z","iopub.execute_input":"2023-05-11T14:41:08.498553Z","iopub.status.idle":"2023-05-11T14:41:10.743149Z","shell.execute_reply.started":"2023-05-11T14:41:08.498514Z","shell.execute_reply":"2023-05-11T14:41:10.741883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop the features are not in the test set\nnew_df =df.drop(['position','click_bool','booking_bool','gross_bookings_usd'],axis=1)\n\n# Define a list of columns to fill missing values for\ncolumns_to_fill = ['visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_review_score', 'prop_location_score2',\n                   'srch_query_affinity_score', 'orig_destination_distance', 'comp1_rate', 'comp1_inv', \n                   'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv', 'comp2_rate_percent_diff', 'comp3_rate', \n                   'comp3_inv', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', \n                   'comp5_rate', 'comp5_inv', 'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n                   'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff',\n                   'comp8_rate', 'comp8_inv', 'comp8_rate_percent_diff']\n\n\n# Loop through each column and fill missing values with the first quartile of the respective country\nfor column in columns_to_fill:\n    # Group the data by country and calculate the first quartile of the column\n    first_quartile_by_country = round(new_df.groupby(\"prop_country_id\")[column].quantile(0.25))\n     # Fill missing values in the column with the first quartile of the respective country\n    new_df[column].fillna(new_df[\"prop_country_id\"].map(first_quartile_by_country), inplace=True)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:10.748788Z","iopub.execute_input":"2023-05-11T14:41:10.749307Z","iopub.status.idle":"2023-05-11T14:41:11.087933Z","shell.execute_reply.started":"2023-05-11T14:41:10.749243Z","shell.execute_reply":"2023-05-11T14:41:11.086564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if still some missing values\nnew_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:11.089999Z","iopub.execute_input":"2023-05-11T14:41:11.090582Z","iopub.status.idle":"2023-05-11T14:41:11.115702Z","shell.execute_reply.started":"2023-05-11T14:41:11.090529Z","shell.execute_reply":"2023-05-11T14:41:11.114477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop the features still have a lot of missing values\nnew_df=new_df.drop(['comp6_rate','comp6_inv','comp6_rate_percent_diff'],axis=1)\n# fill the missing values with forward filling method \nnew_df = new_df.fillna(method='ffill')\nnew_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:11.117183Z","iopub.execute_input":"2023-05-11T14:41:11.117614Z","iopub.status.idle":"2023-05-11T14:41:11.166009Z","shell.execute_reply.started":"2023-05-11T14:41:11.117576Z","shell.execute_reply":"2023-05-11T14:41:11.164592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# perform principal component analysis to reduce the dimensionality\nfrom sklearn.decomposition import PCA\ndef perform_pca(df, pca_dict, dimensions):\n    for feature_set_name, feature_set in pca_dict.items():\n        # Select the columns corresponding to the numeric features\n        data = df[feature_set]        \n        # Drop old features from the original DataFrame\n        df = df.drop(columns=feature_set)        \n        # Perform PCA\n        pca = PCA(n_components=dimensions)\n        transformed_data = pca.fit_transform(data)        \n        # Create a new DataFrame with the transformed data and use the dictionary element name as column name\n        new_df = pd.DataFrame(transformed_data, columns=[f\"{feature_set_name}_{i+1}\" for i in range(dimensions)])        \n        # Concatenate the new DataFrame with the original DataFrame\n        df = pd.concat([df, new_df], axis=1)    \n    return df\n\npca_dict={'comp_rate_pca':['comp1_rate','comp2_rate','comp3_rate',\n                              'comp4_rate','comp5_rate','comp7_rate','comp8_rate'],\n         'comp_inv_pca':['comp1_inv','comp2_inv','comp3_inv',\n                              'comp4_inv','comp5_inv','comp7_inv','comp8_inv'],\n         'comp_rate_percent_diff_pca':['comp1_rate_percent_diff','comp2_rate_percent_diff','comp3_rate_percent_diff',\n                              'comp4_rate_percent_diff','comp5_rate_percent_diff','comp7_rate_percent_diff','comp8_rate_percent_diff']}\nnew_df1= perform_pca(new_df,pca_dict,1)\nnew_df1.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:11.168338Z","iopub.execute_input":"2023-05-11T14:41:11.168744Z","iopub.status.idle":"2023-05-11T14:41:11.836957Z","shell.execute_reply.started":"2023-05-11T14:41:11.168706Z","shell.execute_reply":"2023-05-11T14:41:11.835583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = new_df1\n\n# data = new_df\ndata = data.drop(['srch_id', 'site_id', 'visitor_hist_starrating', 'visitor_hist_adr_usd'], axis=1)\n\n# convert date_time to year, month, day, and hour\ndata['date_time'] = pd.to_datetime(data['date_time'])\ndata['year'] = data['date_time'].dt.year\ndata['month'] = data['date_time'].dt.month\ndata['day'] = data['date_time'].dt.day\ndata['hour'] = data['date_time'].dt.hour\ndata = data.drop('date_time', axis=1)\n\n# # fill missing values with mean\n# data['prop_review_score'] = data['prop_review_score'].fillna(data['prop_review_score'].mean())\n\n# create new features\ndata['log_price_diff'] = data['prop_log_historical_price'] - np.log(data['price_usd']+1)\ndata['price_rank'] = data.groupby('prop_id')['price_usd'].rank(method='dense')\n\ndata['star_rank'] = data.groupby('visitor_location_country_id')['prop_starrating'].rank(method='dense', ascending=False)\n\n# Calculate average price by country and star rating\navg_price = data.groupby(['prop_country_id', 'prop_starrating'])['price_usd'].transform('mean')\n# Calculate price difference and Add price difference as a new feature\ndata['price_diff'] = data['price_usd'] - avg_price\n\n\n# To combine the prop_location_score1 and prop_location_score2 columns to create a new feature that captures \n# the overall location score of the hotel, we can simply add these two columns\ndata['location_score'] = data['prop_location_score1'] + data['prop_location_score2']\n\nprint(data.shape)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:11.838846Z","iopub.execute_input":"2023-05-11T14:41:11.843513Z","iopub.status.idle":"2023-05-11T14:41:12.056624Z","shell.execute_reply.started":"2023-05-11T14:41:11.843443Z","shell.execute_reply":"2023-05-11T14:41:12.055304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = data['location_score'].unique()\nx.sort()\nprint(len(x))\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:12.058198Z","iopub.execute_input":"2023-05-11T14:41:12.058575Z","iopub.status.idle":"2023-05-11T14:41:12.073346Z","shell.execute_reply.started":"2023-05-11T14:41:12.058539Z","shell.execute_reply":"2023-05-11T14:41:12.071780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####################################################","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:12.074969Z","iopub.execute_input":"2023-05-11T14:41:12.075404Z","iopub.status.idle":"2023-05-11T14:41:12.081002Z","shell.execute_reply.started":"2023-05-11T14:41:12.075364Z","shell.execute_reply":"2023-05-11T14:41:12.079599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for balance the target classes \nfrom sklearn.utils import resample\n\n# Separate majority and minority classes\ndf_majority = df[df.booking_bool==0]\ndf_minority = df[df.booking_bool==1]\n\n# Downsample the majority class to match the minority class\ndf_majority_downsampled = resample(df_majority, \n                                   replace=False,   # sample without replacement\n                                   n_samples=len(df_minority),  # match number in minority class\n                                   random_state=42)  # reproducible results\n\n# Combine the downsampled majority class with the minority class\ndf_balanced = pd.concat([df_majority_downsampled, df_minority])\n\n# Check the class distribution\nprint(df_balanced.booking_bool.value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:12.083116Z","iopub.execute_input":"2023-05-11T14:41:12.083582Z","iopub.status.idle":"2023-05-11T14:41:12.127447Z","shell.execute_reply.started":"2023-05-11T14:41:12.083538Z","shell.execute_reply":"2023-05-11T14:41:12.125452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_balanced.to_csv('expedia_balanced.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:12.129561Z","iopub.execute_input":"2023-05-11T14:41:12.129964Z","iopub.status.idle":"2023-05-11T14:41:12.315340Z","shell.execute_reply.started":"2023-05-11T14:41:12.129927Z","shell.execute_reply":"2023-05-11T14:41:12.313894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\n\n# create the user-hotel matrix\nuser_hotel_matrix = df.pivot_table(values='booking_bool', index='visitor_location_country_id', columns='prop_id', fill_value=0)\n# create a nearest neighbors model\nmodel = NearestNeighbors(metric='cosine', algorithm='brute')\n\n# fit the model to the user-hotel matrix\nmodel.fit(user_hotel_matrix)\n\n# get the nearest neighbors for a given user\nuser_index = 1\ndistances, indices = model.kneighbors(user_hotel_matrix.iloc[user_index,:].values.reshape(1, -1), n_neighbors=5)\n\n# use the nearest neighbors to make predictions for the user\nc = user_hotel_matrix.iloc[indices[0],:].mean(axis=0)\n\nprint(c)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:12.317343Z","iopub.execute_input":"2023-05-11T14:41:12.317815Z","iopub.status.idle":"2023-05-11T14:41:21.402744Z","shell.execute_reply.started":"2023-05-11T14:41:12.317771Z","shell.execute_reply":"2023-05-11T14:41:21.401381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_hotel_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:21.404639Z","iopub.execute_input":"2023-05-11T14:41:21.405067Z","iopub.status.idle":"2023-05-11T14:41:21.413483Z","shell.execute_reply.started":"2023-05-11T14:41:21.405027Z","shell.execute_reply":"2023-05-11T14:41:21.412017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.stats import pearsonr\n\n\n# Get a list of all binary variables in the dataset\nbinary_vars = [col for col in df.columns if df[col].dtype == 'int64']\n\n# Get a list of all continuous variables in the dataset\ncontinuous_vars = [col for col in df.columns if df[col].dtype == 'float64']\n\n# Calculate the point-biserial correlation coefficient for each pair of variables\nresults = []\n# for binary_var in binary_vars:\nfor continuous_var in continuous_vars:\n    r_pb, p_value = pearsonr(df['booking_boll'], df[continuous_var])\n    results.append((binary_var, continuous_var, r_pb, p_value))\n\n# Convert the results to a pandas DataFrame and save to a CSV file\ndf_results = pd.DataFrame(results, columns=['binary_var', 'continuous_var', 'r_pb', 'p_value'])\n# df_results.to_csv('expedia_point_biserial_correlations.csv', index=False)\n\n# Print the results for the top 10 correlations\ntop_correlations = df_results.sort_values(by='r_pb', ascending=False).head(10)\nprint(top_correlations)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-11T14:41:21.415373Z","iopub.execute_input":"2023-05-11T14:41:21.415805Z","iopub.status.idle":"2023-05-11T14:41:21.515171Z","shell.execute_reply.started":"2023-05-11T14:41:21.415751Z","shell.execute_reply":"2023-05-11T14:41:21.513582Z"},"trusted":true},"execution_count":null,"outputs":[]}]}